{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50360225",
   "metadata": {},
   "source": [
    "# Transformer model for time series analysis\n",
    "_Using energy and weather data from Spain_\n",
    "\n",
    "The first step is to import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f1228c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import missingno as mno\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "\n",
    "from darts import TimeSeries, concatenate\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import TFTModel, NaiveSeasonal, NaiveDrift, ExponentialSmoothing, TransformerModel\n",
    "from darts.utils.statistics import check_seasonality, extract_trend_and_seasonality, plot_acf, plot_hist\n",
    "from darts.metrics import mape, rmse\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "from darts.utils.utils import ModelMode, SeasonalityMode, TrendMode\n",
    "\n",
    "pd.set_option(\"display.precision\",2)\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3dd1af",
   "metadata": {},
   "source": [
    "# Control settings and constants \n",
    "We set a number of hyperparameters to configure the Transformer model and some other constants that represent additional control settings.\n",
    "Right at the top, note the parameter LOAD.\n",
    "\n",
    "-When it is set to False, the script will train a Transformer model on the training set.\n",
    "\n",
    "-When Load is set to True, the script will not retrain the model. Instead, it will load a previously saved Transformer model from the current working directory — the folder in which you have saved the Jupyter notebook. The lengthy training session will be skipped and the script will proceed to generate forecasts from the loaded model.\n",
    "\n",
    "As soon as a training session completes, the script will automatically save the newly trained model in a tar file. You can modify its file name in line 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab998ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD = False      # True = load previously saved model from disk?  False = (re)train the model\n",
    "SAVE = \"\\_TForm_model10e.pth.tar\"   # file name to save the model under\n",
    "\n",
    "EPOCHS = 200\n",
    "INLEN = 32          # input size\n",
    "FEAT = 32           # d_model = number of expected features in the inputs, up to 512    \n",
    "HEADS = 4           # default 8\n",
    "ENCODE = 4          # encoder layers\n",
    "DECODE = 4          # decoder layers\n",
    "DIM_FF = 128        # dimensions of the feedforward network, default 2048\n",
    "BATCH = 32          # batch size\n",
    "ACTF = \"relu\"       # activation function, relu (default) or gelu\n",
    "SCHLEARN = None     # a PyTorch learning rate scheduler; None = constant rate\n",
    "LEARN = 1e-3        # learning rate\n",
    "VALWAIT = 1         # epochs to wait before evaluating the loss on the test/validation set\n",
    "DROPOUT = 0.1       # dropout rate\n",
    "N_FC = 1            # output size\n",
    "\n",
    "RAND = 42           # random seed\n",
    "N_SAMPLES = 100     # number of times a prediction is sampled from a probabilistic model\n",
    "N_JOBS = 3          # parallel processors to use;  -1 = all processors\n",
    "\n",
    "# default quantiles for QuantileRegression\n",
    "QUANTILES = [0.01, 0.1, 0.2, 0.5, 0.8, 0.9, 0.99]\n",
    "\n",
    "SPLIT = 0.9         # train/test %\n",
    "\n",
    "FIGSIZE = (9, 6)\n",
    "\n",
    "\n",
    "qL1, qL2 = 0.01, 0.10        # percentiles of predictions: lower bounds\n",
    "qU1, qU2 = 1-qL1, 1-qL2,     # upper bounds derived from lower bounds\n",
    "label_q1 = f'{int(qU1 * 100)} / {int(qL1 * 100)} percentile band'\n",
    "label_q2 = f'{int(qU2 * 100)} / {int(qL2 * 100)} percentile band'\n",
    "\n",
    "mpath = os.path.abspath(os.getcwd()) + SAVE     # path and file name to save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a4b403",
   "metadata": {},
   "source": [
    "Download the data and save it into the folder https://www.kaggle.com/datasets/nicholasjhana/energy-consumption-generation-prices-and-weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c330e8d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df0 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menergy_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m dfw0 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweather_features.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdt_iso\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1252\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1254\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:310\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data_length(names, alldata)\n\u001b[0;32m    308\u001b[0m     data \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, (i, v) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(names, data_tups)}\n\u001b[1;32m--> 310\u001b[0m     names, date_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_date_conversions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m     index, names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_index(date_data, alldata, names)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# maybe create a mi on the columns\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:820\u001b[0m, in \u001b[0;36mParserBase._do_date_conversions\u001b[1;34m(self, names, data)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_date_conversions\u001b[39m(\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    814\u001b[0m     names: Sequence[Hashable] \u001b[38;5;241m|\u001b[39m Index,\n\u001b[0;32m    815\u001b[0m     data: Mapping[Hashable, ArrayLike] \u001b[38;5;241m|\u001b[39m DataFrame,\n\u001b[0;32m    816\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Sequence[Hashable] \u001b[38;5;241m|\u001b[39m Index, Mapping[Hashable, ArrayLike] \u001b[38;5;241m|\u001b[39m DataFrame]:\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;66;03m# returns data, columns\u001b[39;00m\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_dates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         data, names \u001b[38;5;241m=\u001b[39m \u001b[43m_process_date_conversion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_date_conv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_date_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_date_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m names, data\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:1189\u001b[0m, in \u001b[0;36m_process_date_conversion\u001b[1;34m(data_dict, converter, parse_spec, index_col, index_names, columns, keep_date_col)\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m     \u001b[38;5;66;03m# Pyarrow engine returns Series which we need to convert to\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m     \u001b[38;5;66;03m# numpy array before converter, its a no-op for other parsers\u001b[39;00m\n\u001b[1;32m-> 1189\u001b[0m     data_dict[colspec] \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolspec\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1191\u001b[0m     new_name, col, old_names \u001b[38;5;241m=\u001b[39m _try_convert_dates(\n\u001b[0;32m   1192\u001b[0m         converter, colspec, data_dict, orig_names\n\u001b[0;32m   1193\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:1070\u001b[0m, in \u001b[0;36m_make_date_converter.<locals>.converter\u001b[1;34m(*date_cols)\u001b[0m\n\u001b[0;32m   1067\u001b[0m strs \u001b[38;5;241m=\u001b[39m parsing\u001b[38;5;241m.\u001b[39mconcat_date_cols(date_cols)\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_datetime_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_datetime_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tools\u001b[38;5;241m.\u001b[39mto_datetime(\n\u001b[0;32m   1081\u001b[0m         parsing\u001b[38;5;241m.\u001b[39mtry_parse_dates(strs, dayfirst\u001b[38;5;241m=\u001b[39mdayfirst), cache\u001b[38;5;241m=\u001b[39mcache_dates\n\u001b[0;32m   1082\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1076\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1074\u001b[0m         result \u001b[38;5;241m=\u001b[39m _convert_and_box_cache(arg, cache_array)\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1076\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1078\u001b[0m     result \u001b[38;5;241m=\u001b[39m convert_listlike(np\u001b[38;5;241m.\u001b[39marray([arg]), \u001b[38;5;28mformat\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:402\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m infer_datetime_format\n\u001b[0;32m    401\u001b[0m utc \u001b[38;5;241m=\u001b[39m tz \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 402\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mobjects_to_datetime64ns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequire_iso8601\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequire_iso8601\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[0;32m    415\u001b[0m     dta \u001b[38;5;241m=\u001b[39m DatetimeArray(result, dtype\u001b[38;5;241m=\u001b[39mtz_to_dtype(tz_parsed))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:2199\u001b[0m, in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object, allow_mixed)\u001b[0m\n\u001b[0;32m   2197\u001b[0m order: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m flags\u001b[38;5;241m.\u001b[39mf_contiguous \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2199\u001b[0m     result, tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mtslib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_to_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2201\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2204\u001b[0m \u001b[43m        \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequire_iso8601\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequire_iso8601\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_mixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_mixed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2208\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mreshape(data\u001b[38;5;241m.\u001b[39mshape, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m   2209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslib.pyx:381\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslib.pyx:641\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslib.pyx:742\u001b[0m, in \u001b[0;36mpandas._libs.tslib._array_to_datetime_object\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\tslibs\\parsing.pyx:281\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1368\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser(parserinfo)\u001b[38;5;241m.\u001b[39mparse(timestr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DEFAULTPARSER\u001b[38;5;241m.\u001b[39mparse(timestr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:640\u001b[0m, in \u001b[0;36mparser.parse\u001b[1;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    637\u001b[0m     default \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mreplace(hour\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, minute\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    638\u001b[0m                                               second\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, microsecond\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 640\u001b[0m res, skipped_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse(timestr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParserError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown string format: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, timestr)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:866\u001b[0m, in \u001b[0;36mparser._parse\u001b[1;34m(self, timestr, dayfirst, yearfirst, fuzzy, fuzzy_with_tokens)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fuzzy_with_tokens:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:383\u001b[0m, in \u001b[0;36mparserinfo.validate\u001b[1;34m(self, res)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m, res):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;66;03m# move to info\u001b[39;00m\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39myear \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 383\u001b[0m         res\u001b[38;5;241m.\u001b[39myear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvertyear(res\u001b[38;5;241m.\u001b[39myear, res\u001b[38;5;241m.\u001b[39mcentury_specified)\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ((res\u001b[38;5;241m.\u001b[39mtzoffset \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res\u001b[38;5;241m.\u001b[39mtzname) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    386\u001b[0m          (res\u001b[38;5;241m.\u001b[39mtzname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m res\u001b[38;5;241m.\u001b[39mtzname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m    387\u001b[0m         res\u001b[38;5;241m.\u001b[39mtzname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTC\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load\n",
    "df0 = pd.read_csv(\"energy_dataset.csv\", header=0, parse_dates=[\"time\"])\n",
    "dfw0 = pd.read_csv(\"weather_features.csv\", header=0, parse_dates=[\"dt_iso\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3bb85",
   "metadata": {},
   "source": [
    "# Display the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3be935",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df0.iloc[[0,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9427bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw0.iloc[[0,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2cf15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup of original sources\n",
    "df1 = df0.copy()\n",
    "dfw1 = dfw0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b00264",
   "metadata": {},
   "source": [
    "# Lets start with the energy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9411593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2359abf1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# datetime\n",
    "df1[\"time\"] = pd.to_datetime(df1[\"time\"], utc=True, infer_datetime_format=True)\n",
    "\n",
    "\n",
    "# any duplicate time periods?\n",
    "print(\"count of duplicates:\",df1.duplicated(subset=[\"time\"], keep=\"first\").sum())\n",
    "\n",
    "\n",
    "df1.set_index(\"time\", inplace=True)\n",
    "\n",
    "\n",
    "# any non-numeric types?\n",
    "print(\"non-numeric columns:\",list(df1.dtypes[df1.dtypes == \"object\"].index))\n",
    "# confirms that we do not need to convert any non numerical variable into  numbers\n",
    "\n",
    "\n",
    "# any missing values?\n",
    "def gaps(df):\n",
    "    if df.isnull().values.any():\n",
    "        print(\"MISSING values:\\n\")\n",
    "        mno.matrix(df)\n",
    "    else:\n",
    "        print(\"no missing values\\n\")\n",
    "gaps(df1)  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dee991d",
   "metadata": {},
   "source": [
    "We observe two empty feature columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the NaN and zero columns, and also the 'forecast' columns\n",
    "df1 = df1.drop(df1.filter(regex=\"forecast\").columns, axis=1, errors=\"ignore\")\n",
    "df1.dropna(axis=1, how=\"all\", inplace=True)\n",
    "df1 = df1.loc[:, (df1!=0).any(axis=0)]\n",
    "\n",
    "\n",
    "# handle missing values in rows of remaining columns. The next lines susbstitute NaNs with interpolated values.\n",
    "df1 = df1.interpolate(method =\"bfill\")\n",
    "# any missing values left?\n",
    "gaps(df1)\n",
    "\n",
    "df1 = df1.loc[:, (df1!=0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd76ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename columns\n",
    "colnames_old = df1.columns\n",
    "colnames_new = [\"gen_bio\", \"gen_lig\", \"gen_gas\", \"gen_coal\", \\\n",
    "                \"gen_oil\", \"gen_hyd_pump\", \"gen_hyd_river\", \"gen_hyd_res\", \\\n",
    "                \"gen_nuc\", \"gen_other\", \"gen_oth_renew\", \"gen_solar\", \\\n",
    "                \"gen_waste\", \"gen_wind\", \"load_actual\", \"price_dayahead\", \\\n",
    "                \"price\"]\n",
    "dict_cols = dict(zip(colnames_old, colnames_new))\n",
    "df1.rename(columns=dict_cols, inplace=True)\n",
    "print(df1.info())\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55418b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert int and float64 columns to float32\n",
    "intcols = list(df1.dtypes[df1.dtypes == np.int64].index)\n",
    "df1[intcols] = df1[intcols].applymap(np.float32)\n",
    "\n",
    "f64cols = list(df1.dtypes[df1.dtypes == np.float64].index)\n",
    "df1[f64cols] = df1[f64cols].applymap(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eead52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(100, figsize=(20, 7))\n",
    "sns.lineplot(x = \"time\", y = \"price\", data = df1, palette=\"coolwarm\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c4599",
   "metadata": {},
   "source": [
    "# Next, weather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b9803",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856e6eb9",
   "metadata": {},
   "source": [
    "We start by converting the “dt_iso” column to datetime format to make it compatible with the datetime index of the energy dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f975fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime\n",
    "dfw1[\"time\"] = pd.to_datetime(dfw1[\"dt_iso\"], utc=True, infer_datetime_format=True)\n",
    "dfw1.set_index(\"time\", inplace=True)\n",
    "\n",
    "\n",
    "# any non-numeric types?\n",
    "print(\"non-numeric columns:\",list(dfw1.dtypes[dfw1.dtypes == \"object\"].index))\n",
    "\n",
    "\n",
    "# any missing values?\n",
    "def gaps(df):\n",
    "    if df.isnull().values.any():\n",
    "        print(\"MISSING values:\\n\")\n",
    "        mno.matrix(df)\n",
    "    else:\n",
    "        print(\"no missing values\\n\")  \n",
    "gaps(dfw1) \n",
    "\n",
    "\n",
    "dfw1.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a621b",
   "metadata": {},
   "source": [
    "**The descriptive statistics reveal some values that evidently represent outliers.**\n",
    "\n",
    "The atmospheric pressure cannot rise above a million millibar without crushing even a nuclear submarine.\n",
    "A wind speed as high as 133 km/h has not been recorded in Spain this century.\n",
    "\n",
    "The temperature columns look outlandish, with values above 300 degrees. But these values are just expressed in kelvin and therefore don’t suggest obvious outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "dfw1.drop([\"rain_3h\", \"weather_id\", \"weather_main\", \"weather_description\", \"weather_icon\"], \n",
    "          inplace=True, axis=1, errors=\"ignore\")\n",
    "\n",
    "\n",
    "# temperature: kelvin to celsius\n",
    "temp_cols = [col for col in dfw1.columns if \"temp\" in col]\n",
    "dfw1[temp_cols] = dfw1[temp_cols].filter(like=\"temp\").applymap(lambda t: t - 273.15)\n",
    "\n",
    "# convert int and float64 columns to float32\n",
    "intcols = list(dfw1.dtypes[dfw1.dtypes == np.int64].index)\n",
    "dfw1[intcols] = dfw1[intcols].applymap(np.float32)\n",
    "\n",
    "f64cols = list(dfw1.dtypes[dfw1.dtypes == np.float64].index)\n",
    "dfw1[f64cols] = dfw1[f64cols].applymap(np.float32)\n",
    "\n",
    "f32cols = list(dfw1.dtypes[dfw1.dtypes == np.float32].index)\n",
    "dfw1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d1757",
   "metadata": {},
   "source": [
    "Next, we need to investigate the outliers which we saw in the descriptive statistics table.\n",
    "\n",
    "The atmospheric pressure is implausibly high on a few days in February 2015. Their normal range is in a narrow band around 1000 millibar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdcf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigate the outliers in the pressure column\n",
    "dfw1[\"pressure\"].nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c0ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigate the outliers in the wind_speed column\n",
    "dfw1[\"wind_speed\"].nlargest(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8fa481",
   "metadata": {},
   "source": [
    "**Boxplots can help us to obtain visual clues on outliers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d529fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# boxplots\n",
    "for i, c in enumerate(f32cols):\n",
    "    sns.boxplot(x=dfw1[c], palette=\"coolwarm\")\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc215e7",
   "metadata": {},
   "source": [
    "Alternatively, we can use seaborn’s distplots to identify outliers.\n",
    "\n",
    "The temperature curve shows a normal seasonal cycle, from freezing winter cold to seething summer heat. Without a comparison with third-party data, we won’t discern obvious temperate outliers.\n",
    "\n",
    "But the extreme kurtosis and the skew of the pressure and wind speed distributions reveal the existence of outliers in their right tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61eaa26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# or use distplot to visualize outliers\n",
    "fig = plt.figure(figsize=(5, 4)) \n",
    "ax = sns.distplot(dfw1[\"temp\"])\n",
    "xmin = dfw1[\"temp\"].min()\n",
    "xmax = dfw1[\"temp\"].max()  \n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_title(\"temp\");\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = sns.distplot(dfw1[\"pressure\"])\n",
    "xmin = dfw1[\"pressure\"].min()\n",
    "xmax = dfw1[\"pressure\"].max()  \n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_title(\"pressure\");\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = sns.distplot(dfw1[\"wind_speed\"])\n",
    "xmin = dfw1[\"wind_speed\"].min()\n",
    "xmax = dfw1[\"wind_speed\"].max()  \n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_title(\"wind_speed\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989cbf2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# treatment of outliers: replace with NaN, then interpolate\n",
    "dfw1[\"pressure\"].where( dfw1[\"pressure\"] <= 1050, inplace=True)\n",
    "dfw1[\"pressure\"].where( dfw1[\"pressure\"] >= 948, inplace=True)\n",
    "dfw1[\"wind_speed\"].where( dfw1[\"wind_speed\"] <= 120, inplace=True)\n",
    "dfw1[\"clouds_all\"].where( dfw1[\"clouds_all\"] <= 40, inplace=True)\n",
    "dfw1 = dfw1.interpolate(method =\"bfill\")\n",
    "\n",
    "sns.boxplot(x=dfw1[\"pressure\"], palette=\"coolwarm\")\n",
    "plt.show();\n",
    "sns.boxplot(x=dfw1[\"wind_speed\"], palette=\"coolwarm\")\n",
    "plt.show();\n",
    "sns.boxplot(x=dfw1[\"clouds_all\"], palette=\"coolwarm\")\n",
    "plt.show();\n",
    "\n",
    "dfw1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50a6b18",
   "metadata": {},
   "source": [
    "Now, lets make sure that both data sets have the same time index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441989b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end of energy and weather time series \n",
    "print(\"earliest weather time period:\", dfw1.index.min())\n",
    "print(\"latest weather time period:\", dfw1.index.max())\n",
    "\n",
    "print(\"earliest energy time period:\", df1.index.min())\n",
    "print(\"latest energy time period:\", df1.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities in weather data\n",
    "cities = dfw1[\"city_name\"].unique()\n",
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224475a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate time periods\n",
    "print(\"count of duplicates before treatment:\",dfw1.duplicated(subset=[\"dt_iso\", \"city_name\"], keep=\"first\").sum())\n",
    "\n",
    "dfw1 = dfw1.drop_duplicates(subset=[\"dt_iso\", \"city_name\"], keep=\"first\")\n",
    "dfw1.reset_index()\n",
    "print(\"count of duplicates after treatment:\",dfw1.duplicated(subset=[\"dt_iso\", \"city_name\"], keep=\"first\").sum())\n",
    "\n",
    "# set datetime index\n",
    "dfw1[\"time\"] = pd.to_datetime(dfw1[\"dt_iso\"], utc=True, infer_datetime_format=True)\n",
    "dfw1.set_index(\"time\", inplace=True)\n",
    "dfw1.drop(\"dt_iso\", inplace=True, axis=1)\n",
    "\n",
    "\n",
    "print(\"size of energy dataframe:\", df1.shape[0])\n",
    "dfw1_city = dfw1.groupby(\"city_name\").count()\n",
    "dfw1_city\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c051a",
   "metadata": {},
   "source": [
    "We isolate each city’s weather records in a dataframe of its own to prepare for the merger of the energy and weather records. The dictionary comprehension in line 2 creates the city-specific dataframes.\n",
    "\n",
    "The city names serve as the dictionary keys. When we select, for instance, the key “Bilbao”, we can retrieve the Basque city’s dataframe from the dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4596131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of weather observations by city\n",
    "print(\"size of energy dataframe:\", df1.shape[0])\n",
    "\n",
    "dfw1[\"city_name\"] = dfw1[\"city_name\"].replace(\" Barcelona\", \"Barcelona\")   # remove space in name\n",
    "dfw1_city = dfw1.groupby(\"city_name\")\n",
    "print(\"size of city groups in weather dataframe:\")\n",
    "dfw1_city.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0223aff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# separate the cities: a weather dataframe for each of them\n",
    "# One must redefine dfw1_city in order to remove the .count() command. \n",
    "\n",
    "dict_city_weather = {city:df_city for city,df_city in dfw1_city}\n",
    "dict_city_weather.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b82d1",
   "metadata": {},
   "source": [
    "**Two examples: Barcelona and Bilbao**\n",
    "\n",
    "_Note that \" Barcelona\" has a space at the begining of the word_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb64367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: Bilbao weather dataframe\n",
    "dfw_Bilbao = dict_city_weather.get(\"Bilbao\")\n",
    "print(\"Bilbao weather:\")\n",
    "dfw_Bilbao.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1baf7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# example: Barcelona weather dataframe\n",
    "dfw_Barcelona = dict_city_weather.get(\"Barcelona\")\n",
    "print(\"Barcelona weather:\")\n",
    "dfw_Barcelona.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9cb7f0",
   "metadata": {},
   "source": [
    "# Merger of the Energy and Weather Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea09bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the energy and weather dataframes\n",
    "df2 = df1.copy()\n",
    "for city,df in dict_city_weather.items():\n",
    "    city_name = str(city) + \"_\"\n",
    "    df = df.add_suffix(\"_{}\".format(city))\n",
    "    df2 = pd.concat([df2, df], axis=1)\n",
    "    df2.drop(\"city_name_\" + city, inplace=True, axis=1)\n",
    "print(df2.info())\n",
    "df2.iloc[[0,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_headers = list(df2.columns.values)\n",
    "print(\"The Column Header :\", column_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2104c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any null values?\n",
    "print(\"any missing values?\", df2.isnull().values.any())\n",
    "\n",
    "# any ducplicate time periods?\n",
    "print(\"count of duplicates:\", df2.duplicated(keep=\"first\").sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d3a71",
   "metadata": {},
   "source": [
    "In an out-of-sample forecast, we want to estimate the electricity prices over the next few hours. It is doubtful that historical prices, or any of the feature variables, have formed patterns that have persisted over several years and will influence the prices we will observe 12 hours from now. I choose to limit the source data to the 8,760 hours of the final year, January to December 2018. Otherwise, the training time would have been multiplied. The training would take half a day or night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the dataframe's date range\n",
    "df2 = df2[df2.index >= \"2018-01-01 00:00:00+00:00\"]\n",
    "df2.iloc[[0,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlations of features with price\n",
    "df_corr = df2.corr(method=\"pearson\")\n",
    "print(df_corr.shape)\n",
    "print(\"correlation with price:\")\n",
    "df_corrP = pd.DataFrame(df_corr[\"price\"].sort_values(ascending=False))\n",
    "df_corrP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest absolute correlations with price\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "df_corrH = df_corrP[np.abs(df_corrP[\"price\"]) > 0.3]\n",
    "df_corrH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method: correlation matrix as heatmap\n",
    "def corr_heatmap(df):\n",
    "    idx = df.corr().sort_values(\"price\", ascending=False).index\n",
    "    df_sorted = df.loc[:, idx]  # sort dataframe columns by their correlation \n",
    "\n",
    "    #plt.figure(figsize = (15,15))\n",
    "    sns.set(font_scale=0.75)\n",
    "    ax = sns.heatmap(df_sorted.corr().round(3), \n",
    "            annot=True, \n",
    "            square=True, \n",
    "            linewidths=.75, cmap=\"coolwarm\", \n",
    "            fmt = \".2f\", \n",
    "            annot_kws = {\"size\": 11})\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.title(\"correlation matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ddd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call helper function to plot correlation heatmap\n",
    "df3 = df2.copy()\n",
    "df3 = df3[df_corrH.index]\n",
    "plt.figure(figsize = (15,15))\n",
    "corr_heatmap(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4a616",
   "metadata": {},
   "source": [
    "We can choose between two approaches.\n",
    "\n",
    "The first one proceeds with those 28 original features which are at least moderately correlated with the prices. The alternative would be a principal component analysis that reduces the 66 features to, hopefully, just a handful of their linear combinations.\n",
    "\n",
    "Lets behind with the principal component solutions (PCA). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84435f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA:\n",
    "# dataframe with feature columns only, without actual price\n",
    "df3 = df2.copy()\n",
    "df_feat = df3.loc[:, df3.columns != \"price\"]\n",
    "#print(df_feat.info())\n",
    "df_feat = MinMaxScaler().fit_transform(df_feat)\n",
    "\n",
    "# principal components among features\n",
    "pca = PCA(n_components=30)\n",
    "res_pca = pca.fit_transform(df_feat)\n",
    "\n",
    "# scree plot\n",
    "features = range(pca.n_components_)\n",
    "plt.figure(figsize = (25,10))\n",
    "plt.bar(features, pca.explained_variance_ratio_, color=\"blue\")\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"PCA features\", fontsize=15)\n",
    "plt.ylabel(\"% variance explained\", fontsize=20)\n",
    "plt.xticks(features, fontsize=20)\n",
    "plt.yticks(ticks=np.arange(0.0, 1.0001, 0.1), fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a748de70",
   "metadata": {},
   "source": [
    "The principal component analysis demonstrates that as many as 30 components would be needed to reach a cumulative variance ratio higher than 90%. Thus, PCA does not condense the feature columns as much as we would wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca688480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect principal components in a dataframe\n",
    "df_pca = pd.DataFrame(res_pca)\n",
    "df_pca = df_pca.add_prefix(\"pca\")\n",
    "\n",
    "\n",
    "df3 = df2.copy()\n",
    "col = df3.pop(\"price\")\n",
    "df3.insert(0, col.name, col)     # move price column to the left\n",
    "\n",
    "# drop the feature columns which the components summarize\n",
    "df3.reset_index(inplace=True)\n",
    "df3 = pd.concat([df3, df_pca], axis=1)\n",
    "for col in df3.columns:\n",
    "    if col != \"price\" and col != \"time\" and \"pca\" not in col:\n",
    "        del df3[col]\n",
    "df3.set_index(\"time\", inplace=True)\n",
    "df3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc4596d",
   "metadata": {},
   "source": [
    "Let’s compute their correlations with the price level and limit the output to those components which have at least 10% correlation with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145421ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare correlation matrix for principal components  \n",
    "df_corr = df3.corr(method=\"pearson\")\n",
    "print(\"principal components with at least modest correlation to price:\")\n",
    "df_corrP = pd.DataFrame(df_corr[\"price\"].sort_values(ascending=False))\n",
    "\n",
    "# absolute correlations with price > 10%\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "df_corrH = df_corrP[np.abs(df_corrP[\"price\"]) > 0.10]\n",
    "df_corrH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a44c37",
   "metadata": {},
   "source": [
    "Lets define a helper function to make things easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b64b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize correlations with price     \n",
    "df4 = df3.copy()\n",
    "df4 = df4[df_corrH.index]   # keep the components with at least modest correlations\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "corr_heatmap(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3934e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional datetime columns to discern patterns along time axis\n",
    "df4[\"month\"] = df4.index.month\n",
    "\n",
    "df4[\"wday\"] = df4.index.dayofweek\n",
    "dict_days = {0:\"1_Mon\", 1:\"2_Tue\", 2:\"3_Wed\", 3:\"4_Thu\", 4:\"5_Fri\", 5:\"6_Sat\", 6:\"7_Sun\"}\n",
    "df4[\"weekday\"] = df4[\"wday\"].apply(lambda x: dict_days[x])\n",
    "\n",
    "df4[\"hour\"] = df4.index.hour\n",
    "\n",
    "df4 = df4.astype({\"hour\":float, \"wday\":float, \"month\": float})\n",
    "\n",
    "df4.iloc[[0, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b7742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot table: weekdays in months\n",
    "piv = pd.pivot_table(   df4, \n",
    "                        values=\"price\", \n",
    "                        index=\"month\", \n",
    "                        columns=\"weekday\", \n",
    "                        aggfunc=\"mean\", \n",
    "                        margins=True, margins_name=\"Avg\", \n",
    "                        fill_value=0)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "plt.figure(figsize = (7,15))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(piv.round(0), annot=True, square = True, \\\n",
    "            linewidths=.75, cmap=\"coolwarm\", fmt = \".0f\", annot_kws = {\"size\": 12})\n",
    "plt.title(\"price by weekday by month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f40accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot table: hours in weekdays\n",
    "piv = pd.pivot_table(   df4, \n",
    "                        values=\"price\", \n",
    "                        index=\"hour\", \n",
    "                        columns=\"weekday\", \n",
    "                        aggfunc=\"mean\", \n",
    "                        margins=True, margins_name=\"Avg\", \n",
    "                        fill_value=0)\n",
    "pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "plt.figure(figsize = (7,20))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(piv.round(0), annot=True, square = True, \\\n",
    "            linewidths=.75, cmap=\"coolwarm\", fmt = \".0f\", annot_kws = {\"size\": 12})\n",
    "plt.title(\"price by hour by weekday\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16bbfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with price and feature columns only\n",
    "df4.drop([\"weekday\", \"month\", \"wday\", \"hour\"], inplace=True, axis=1)\n",
    "# print(df4.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f43908",
   "metadata": {},
   "source": [
    "# Time series object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c6f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time series object for the target variable\n",
    "ts_P = TimeSeries.from_series(df4[\"price\"]) \n",
    "\n",
    "# check attributes of the time series\n",
    "print(\"components:\", ts_P.components)\n",
    "print(\"duration:\",ts_P.duration)\n",
    "print(\"frequency:\",ts_P.freq)\n",
    "print(\"frequency:\",ts_P.freq_str)\n",
    "print(\"has date time index? (or else, it must have an integer index):\",ts_P.has_datetime_index)\n",
    "print(\"deterministic:\",ts_P.is_deterministic)\n",
    "print(\"univariate:\",ts_P.is_univariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a time series object for the feature columns\n",
    "df_covF = df4.loc[:, df4.columns != \"price\"]\n",
    "ts_covF = TimeSeries.from_dataframe(df_covF)\n",
    "\n",
    "\n",
    "# check attributes of the time series\n",
    "print(\"components (columns) of feature time series:\", ts_covF.components)\n",
    "print(\"duration:\",ts_covF.duration)\n",
    "print(\"frequency:\",ts_covF.freq)\n",
    "print(\"frequency:\",ts_covF.freq_str)\n",
    "print(\"has date time index? (or else, it must have an integer index):\",ts_covF.has_datetime_index)\n",
    "print(\"deterministic:\",ts_covF.is_deterministic)\n",
    "print(\"univariate:\",ts_covF.is_univariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: operating with time series objects:\n",
    "# we can also create a 3-dimensional numpy array from a time series object\n",
    "# 3 dimensions: time (rows) / components (columns) / samples\n",
    "ar_covF = ts_covF.all_values()\n",
    "print(type(ar_covF))\n",
    "ar_covF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: operating with time series objects:\n",
    "# we can also create a pandas series or dataframe from a time series object\n",
    "df_covF = ts_covF.pd_dataframe()\n",
    "type(df_covF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split and scaling of target variable\n",
    "ts_train, ts_test = ts_P.split_after(SPLIT)\n",
    "print(\"training start:\", ts_train.start_time())\n",
    "print(\"training end:\", ts_train.end_time())\n",
    "print(\"training duration:\",ts_train.duration)\n",
    "print(\"test start:\", ts_test.start_time())\n",
    "print(\"test end:\", ts_test.end_time())\n",
    "print(\"test duration:\", ts_test.duration)\n",
    "\n",
    "\n",
    "scalerP = Scaler()\n",
    "scalerP.fit_transform(ts_train)\n",
    "ts_ttrain = scalerP.transform(ts_train)\n",
    "ts_ttest = scalerP.transform(ts_test)    \n",
    "ts_t = scalerP.transform(ts_P)\n",
    "\n",
    "# make sure data are of type float\n",
    "ts_t = ts_t.astype(np.float32)\n",
    "ts_ttrain = ts_ttrain.astype(np.float32)\n",
    "ts_ttest = ts_ttest.astype(np.float32)\n",
    "\n",
    "print(\"first and last row of scaled price time series:\")\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "ts_t.pd_dataframe().iloc[[0,-1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606bb27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split and scaling of feature covariates\n",
    "covF_train, covF_test = ts_covF.split_after(SPLIT)\n",
    "\n",
    "scalerF = Scaler()\n",
    "scalerF.fit_transform(covF_train)\n",
    "covF_ttrain = scalerF.transform(covF_train) \n",
    "covF_ttest = scalerF.transform(covF_test)   \n",
    "covF_t = scalerF.transform(ts_covF)  \n",
    "\n",
    "# make sure data are of type float\n",
    "covF_ttrain = ts_ttrain.astype(np.float32)\n",
    "covF_ttest = ts_ttest.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering - create time covariates: hour, weekday, month, year, country-specific holidays\n",
    "covT = datetime_attribute_timeseries(ts_P.time_index, attribute=\"hour\", one_hot=False)\n",
    "covT = covT.stack(datetime_attribute_timeseries(ts_P.time_index, attribute=\"day_of_week\", one_hot=False))\n",
    "covT = covT.stack(datetime_attribute_timeseries(ts_P.time_index, attribute=\"month\", one_hot=False))\n",
    "covT = covT.stack(datetime_attribute_timeseries(ts_P.time_index, attribute=\"year\", one_hot=False))\n",
    "\n",
    "covT = covT.add_holidays(country_code=\"ES\")\n",
    "covT = covT.astype(np.float32)\n",
    "\n",
    "\n",
    "# train/test split\n",
    "covT_train, covT_test = covT.split_after(SPLIT)\n",
    "\n",
    "\n",
    "# rescale the covariates: fitting on the training set\n",
    "scalerT = Scaler()\n",
    "scalerT.fit(covT_train)\n",
    "covT_ttrain = scalerT.transform(covT_train)\n",
    "covT_ttest = scalerT.transform(covT_test)\n",
    "covT_t = scalerT.transform(covT)\n",
    "covT_t = covT_t.astype(np.float32)\n",
    "\n",
    "\n",
    "pd.options.display.float_format = '{:.0f}'.format\n",
    "print(\"first and last row of unscaled time covariates:\")\n",
    "covT.pd_dataframe().iloc[[0,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb0f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine feature covariates and time covariates in a single time series object\n",
    "ts_cov = ts_covF.concatenate(covT, axis=1)                      # unscaled F+T\n",
    "cov_t = covF_t.concatenate(covT_t, axis=1)                      # scaled F+T\n",
    "cov_ttrain = covF_ttrain.concatenate(covT_ttrain, axis=1)       # scaled F+T training\n",
    "\n",
    "print(\"first and last row of unscaled covariates:\")\n",
    "ts_cov.pd_dataframe().iloc[[0,-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104774e9",
   "metadata": {},
   "source": [
    "# Training the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b15b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "                    input_chunk_length = INLEN,\n",
    "                    output_chunk_length = N_FC,\n",
    "                    batch_size = BATCH,\n",
    "                    n_epochs = EPOCHS,\n",
    "                    model_name = \"Transformer_price\",\n",
    "                    nr_epochs_val_period = VALWAIT,\n",
    "                    d_model = FEAT,\n",
    "                    nhead = HEADS,\n",
    "                    num_encoder_layers = ENCODE,\n",
    "                    num_decoder_layers = DECODE,\n",
    "                    dim_feedforward = DIM_FF,\n",
    "                    dropout = DROPOUT,\n",
    "                    activation = ACTF,\n",
    "                    random_state=RAND,\n",
    "                    likelihood=QuantileRegression(quantiles=QUANTILES), \n",
    "                    optimizer_kwargs={'lr': LEARN},\n",
    "                    add_encoders={\"cyclic\": {\"future\": [\"hour\", \"dayofweek\", \"month\"]}},\n",
    "                    save_checkpoints=True,\n",
    "                    force_reset=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training: load a saved model or (re)train\n",
    "if LOAD:\n",
    "    print(\"have loaded a previously saved model from disk:\" + mpath)\n",
    "    model = TransformerModel.load_model(mpath)                            # load previously model from disk \n",
    "else:\n",
    "    model.fit(  ts_ttrain, \n",
    "                past_covariates=cov_t, \n",
    "                verbose=True)\n",
    "    print(\"have saved the model after training:\", mpath)\n",
    "    model.save_model(mpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaaffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to current working directory\n",
    "print(\"saved model:\", mpath)\n",
    "model.save_model(mpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecfda0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing: generate predictions\n",
    "ts_tpred = model.predict(n=len(ts_ttest),\n",
    "                         num_samples=N_SAMPLES,\n",
    "                            n_jobs=N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve forecast series for chosen quantiles, \n",
    "# inverse-transform each series,\n",
    "# insert them as columns in a new dataframe dfY\n",
    "q50_RMSE = np.inf\n",
    "q50_MAPE = np.inf\n",
    "ts_q50 = None\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "dfY = pd.DataFrame()\n",
    "dfY[\"Actual\"] = TimeSeries.pd_series(ts_test)\n",
    "\n",
    "\n",
    "# helper function: get forecast values for selected quantile q and insert them in dataframe dfY\n",
    "def predQ(ts_t, q):\n",
    "    ts_tq = ts_t.quantile_timeseries(q)\n",
    "    ts_q = scalerP.inverse_transform(ts_tq)\n",
    "    s = TimeSeries.pd_series(ts_q)\n",
    "    header = \"Q\" + format(int(q*100), \"02d\")\n",
    "    dfY[header] = s\n",
    "    if q==0.5:\n",
    "        ts_q50 = ts_q\n",
    "        q50_RMSE = rmse(ts_q50, ts_test)\n",
    "        q50_MAPE = mape(ts_q50, ts_test) \n",
    "        print(\"RMSE:\", f'{q50_RMSE:.2f}')\n",
    "        print(\"MAPE:\", f'{q50_MAPE:.2f}')\n",
    "  \n",
    "    \n",
    "# call helper function predQ, once for every quantile\n",
    "_ = [predQ(ts_tpred, q) for q in QUANTILES]\n",
    "\n",
    "# move Q50 column to the left of the Actual column\n",
    "col = dfY.pop(\"Q50\")\n",
    "dfY.insert(1, col.name, col)\n",
    "dfY.iloc[np.r_[0:2, -2:0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(100, figsize=(20, 7))\n",
    "sns.set(font_scale=1.3)\n",
    "p = sns.lineplot(x=\"time\", y=\"Q50\", data=dfY, palette=\"coolwarm\")\n",
    "sns.lineplot(x=\"time\", y=\"Actual\", data=dfY, palette=\"coolwarm\")\n",
    "plt.legend(labels=[\"forecast median price Q50\", \"actual price\"])\n",
    "p.set_ylabel(\"price\")\n",
    "p.set_xlabel(\"\")\n",
    "p.set_title(\"energy price\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# choose forecast horizon: k hours beyond end of test set\n",
    "k = 12   \n",
    "\n",
    "n_FC = k + len(ts_ttest)   # length of test set + k hours\n",
    "print(\"forecast beyond end of training set:\", n_FC, \n",
    "      \"hours beyond\", ts_ttrain.end_time())\n",
    "\n",
    "# last 24 hours of feature covariates available => copy them to future 24 hours:\n",
    "covF_t_fut = covF_t.concatenate(other=covF_t.tail(size=24), \n",
    "                                    ignore_time_axes=True)\n",
    "covF_t_fut.pd_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "covT_t.pd_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9576fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine feature and time covariates:\n",
    "# cov_t_fut = covF_t_fut.concatenate(covT_t.slice_intersect(covF_t_fut), axis=1) \n",
    "# cov_t_fut.pd_dataframe().iloc[[0,-1]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Im having trouble putting together here. Lets try without PCA first to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8a32e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
